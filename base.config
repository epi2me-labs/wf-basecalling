params {
    out_dir = "output"
    wf {
        basecaller_container = "ontresearch/dorado"
        container_sha_basecalling = "shae9327ad17e023b76e4d27cf287b6b9d3a271092b"
        bonito_container = "ontresearch/bonito"
        bonito_sha = "shaea43ca2333f91fa78a823f640ba158e4268f1f98"
        common_sha = "sha72f3517dd994984e0e2da0b97cb3f23f8540be4b"
    }
}


// used by default for "standard" (docker) and singularity profiles,
// other profiles may override.
process {
    withLabel:wf_basecalling {
        container = "${params.wf.basecaller_container}:${params.wf.container_sha_basecalling}"
    }
    withLabel:wf_common {
        container = "ontresearch/wf-common:${params.wf.common_sha}"
    }

    shell = ['/bin/bash', '-euo', 'pipefail']

    // by default GPU tasks will run in serial to avoid GPU management.
    // cluster and cloud users can remove this with -profile discrete_gpus.
    // we use profiles to handle this as maxForks cannot be set dynamically
    // see https://github.com/nextflow-io/nextflow/discussions/3806 and CW-1857
    withLabel:gpu {
        maxForks = 1
    }
}

profiles {
    // the "standard" profile is used implicitely by nextflow
    // if no other profile is given on the CLI
    standard {
        docker {
            enabled = true
            // this ensures container is run as host user and group, but
            //    also adds host user to the within-container group
            runOptions = "--user \$(id -u):\$(id -g) --group-add 100"
        }
        process."withLabel:gpu".containerOptions = "--gpus all"
    }

    // using singularity instead of docker
    singularity {
        singularity {
            enabled = true
            autoMounts = true
            //envWhitelist = "" // if your cluster sets a variable to indicate which GPU has been assigned you will want to allow it here
        }
        process."withLabel:gpu".containerOptions = "--nv"
    }


    // keep stub conda profile to prevent unknown profile warning so users get a better error
    conda {
        conda.enabled = true
    }


    // Using AWS batch.
    // May need to set aws.region and aws.batch.cliPath
    awsbatch {
        process {
            executor = 'awsbatch'
            queue = "${params.aws_queue}"
            memory = "16 GB" // likely not enough!
            withLabel:wf_common {
                container = "${params.aws_image_prefix}-wf-common:${params.wf.common_sha}"
            }
            shell = ['/bin/bash', '-euo', 'pipefail']
            
            // lift limit on simultaneous gpu jobs for cloud
            // and ensure that the host mounts relevant driver bobbins inside the container
            withLabel:gpu {
                maxForks = null
                containerOptions = "-e NVIDIA_DRIVER_CAPABILITIES=compute,utility --gpus all"
            }
            withLabel:wf_basecalling {
                container = "${params.aws_image_prefix}-dorado:${params.wf.container_sha_basecalling}"
            }
            withLabel:wf_bonito {
                container = "${params.aws_image_prefix}-bonito:${params.wf.bonito_sha}"
            }
        }
    }

    // local profile for simplified development testing
    local {
        process.executor = 'local'
    }

    // lift limit on simultaneous gpu jobs
    discrete_gpus {
        process."withLabel:gpu".maxForks = null
    }
}


timeline {
  enabled = true
  file = "${params.out_dir}/execution/timeline.html"
  overwrite = true
}
report {
  enabled = true
  file = "${params.out_dir}/execution/report.html"
  overwrite = true
}
trace {
  enabled = true
  file = "${params.out_dir}/execution/trace.txt"
  overwrite = true
}

env {
    PYTHONNOUSERSITE = 1
    JAVA_TOOL_OPTIONS = "-Xlog:disable -Xlog:all=warning:stderr"
}

cleanup = true
