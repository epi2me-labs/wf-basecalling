//
// Notes to End Users.
//
// The workflow should run without editing this configuration file,
// however there may be instances in which you wish to edit this
// file for compute performance or other reasons. Please see:
//
//   https://nextflow.io/docs/latest/config.html#configuration
//
// for further help editing this file.


params {
    help = false
    version = false
    aws_image_prefix = null
    aws_queue = null
    disable_ping = false

    monochrome_logs = false
    validate_params = true
    show_hidden_params = false
    schema_ignore_params = 'show_hidden_params,validate_params,monochrome_logs,aws_queue,aws_image_prefix,wf'

    // I/O
    input = null
    ref = null
    out_dir = "output"
    sample_name = "SAMPLE"

    // basecalling
    /// common
    basecaller_chunk_size = 25
    basecaller_cfg = null
    basecaller_args = null
    basecaller_basemod_threads = 2
    duplex = false
    cuda_device = "cuda:all"
    ubam_map_threads = 8
    ubam_sort_threads = 3
    ubam_bam2fq_threads = 1
    merge_threads = 4
    stats_threads = 4
    basecaller_model_path = null
    remora_model_path = null
    qscore_filter = 10
    /// dorado
    remora_cfg = null
    dorado_ext = "pod5"
    /// wf-basecalling
    fastq_only = false
    output_bam = false
    output_pod5 = false
    /// Stream input
    watch_path = false
    read_limit = null

    // Defaults    
    max_memory = '256.GB'
    max_cpus = 32
    max_time = '48.h'

    wf {
        basecaller_container = "ontresearch/dorado"
        container_sha = "sha06c3f67311887d7976509748ae9357788fafce94"
        common_sha = "sha0a6dc21fac17291f4acb2e0f67bcdec7bf63e6b7"
        example_cmd = [
            "--basecaller_cfg 'dna_r10.4.1_e8.2_400bps_hac@v4.1.0'",
            "--dorado_ext 'pod5'",
            "--input 'wf-basecalling-demo/input'",
            "--ref 'wf-basecalling-demo/GCA_000001405.15_GRCh38_no_alt_analysis_set.fasta'",
            "--remora_cfg 'dna_r10.4.1_e8.2_400bps_hac@v4.1.0_5mCG_5hmCG@v2'",
        ]
        agent = null
    }
}

manifest {
    name            = 'epi2me-labs/wf-basecalling'
    author          = 'Oxford Nanopore Technologies'
    homePage        = 'https://github.com/epi2me-labs/wf-basecalling'
    description     = 'Helper workflow for basecalling ONT reads.'
    mainScript      = 'main.nf'
    nextflowVersion = '>=23.04.2'
    version         = '1.1.1'
}

epi2melabs {
    tags = "basecalling,utility"
    icon = "faTty"
}

executor {
    $local {
        cpus = 12
        memory = "16 GB" // likely not enough!
    }
}


// used by default for "standard" (docker) and singularity profiles,
// other profiles may override.
process {
    withLabel:wf_basecalling {
        container = "${params.wf.basecaller_container}:${params.wf.container_sha}"
    }
    withLabel:wf_common {
        container = "ontresearch/wf-common:${params.wf.common_sha}"
    }

    shell = ['/bin/bash', '-euo', 'pipefail']

    // by default GPU tasks will run in serial to avoid GPU management.
    // cluster and cloud users can remove this with -profile discrete_gpus.
    // we use profiles to handle this as maxForks cannot be set dynamically
    // see https://github.com/nextflow-io/nextflow/discussions/3806 and CW-1857
    withLabel:gpu {
        maxForks = 1
    }
}


profiles {
    // the "standard" profile is used implicitely by nextflow
    // if no other profile is given on the CLI
    standard {
        docker {
            enabled = true
            // this ensures container is run as host user and group, but
            //    also adds host user to the within-container group
            runOptions = "--user \$(id -u):\$(id -g) --group-add 100"
        }
        process."withLabel:gpu".containerOptions = "--gpus all"
    }

    // using singularity instead of docker
    singularity {
        singularity {
            enabled = true
            autoMounts = true
            //envWhitelist = "" // if your cluster sets a variable to indicate which GPU has been assigned you will want to allow it here
        }
        process."withLabel:gpu".containerOptions = "--nv"
    }


    // keep stub conda profile to prevent unknown profile warning so users get a better error
    conda {
        conda.enabled = true
    }


    // Using AWS batch.
    // May need to set aws.region and aws.batch.cliPath
    awsbatch {
        process {
            executor = 'awsbatch'
            queue = "${params.aws_queue}"
            memory = "16 GB" // likely not enough!
            withLabel:wf_common {
                container = "${params.aws_image_prefix}-wf-common:${params.wf.common_sha}-root"
            }
            shell = ['/bin/bash', '-euo', 'pipefail']
        }
    }

    // local profile for simplified development testing
    local {
        process.executor = 'local'
    }

    // lift limit on simultaneous gpu jobs
    discrete_gpus {
        process."withLabel:gpu".maxForks = null
    }
}


timeline {
  enabled = true
  file = "${params.out_dir}/execution/timeline.html"
  overwrite = true
}
report {
  enabled = true
  file = "${params.out_dir}/execution/report.html"
  overwrite = true
}
trace {
  enabled = true
  file = "${params.out_dir}/execution/trace.txt"
  overwrite = true
}

env {
    PYTHONNOUSERSITE = 1
}

cleanup = true
